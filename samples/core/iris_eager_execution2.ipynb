{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris_eager_execution2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "JtEZ1pCPn--z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Iris Dataset with Eager Execution\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yNr7H-AIoLOR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup program"
      ]
    },
    {
      "metadata": {
        "id": "6qoYFqQ89aV3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the latest version of TensorFlow\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jBmKxLVy9Uhg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1J3AuPBT9gyR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configure imports and eager execution\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g4Wzg69bnwK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Px6KAg0Jowz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import and parse the training dataset\n",
        "\n",
        "\n",
        "### Download the dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J6c7uEU9rjRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "\n",
        "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
        "                                           origin=train_dataset_url)\n",
        "\n",
        "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnX1-aLors4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inspect the data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FQvb_JYdrpPm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head -n5 {train_dataset_fp}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Edhevw7exl6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# column order in CSV file\n",
        "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
        "\n",
        "feature_names = column_names[:-1]\n",
        "label_name = column_names[-1]\n",
        "\n",
        "print(\"Features: {}\".format(feature_names))\n",
        "print(\"Label: {}\".format(label_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVNlJlUOhkoX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqPkQExM2Pwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a `tf.data.Dataset`\n",
        "\n",
        "TensorFlow's [Dataset API](https://www.tensorflow.org/programmers_guide/datasets) handles many common cases for loading data into a model. This is a high-level API for reading data and transforming it into a form used for training. See the [Datasets Quick Start guide](https://www.tensorflow.org/get_started/datasets_quickstart) for more information.\n",
        "\n",
        "\n",
        "Since the dataset is a CSV-formatted text file, use the the [make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/contrib/data/make_csv_dataset) function to parse the data into a suitable format. Since this function generates data for training models, the default behavior is to shuffle the data (`shuffle=True, shuffle_buffer_size=10000`), and repeat the dataset forever (`num_epochs=None`). We also set the [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size) parameter."
      ]
    },
    {
      "metadata": {
        "id": "WsxHnz1ebJ2S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.contrib.data.make_csv_dataset(\n",
        "    train_dataset_fp,\n",
        "    batch_size, \n",
        "    column_names=column_names,\n",
        "    label_name=label_name,\n",
        "    num_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gB_RSn62c-3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `make_csv_dataset` function returns a `tf.data.Dataset` of `(features, label)` pairs, where `features` is a dictionary: `{'feature_name': value}`\n",
        "\n",
        "With eager execution enabled, these `Dataset` objects are iterable. Let's look at a batch of features:"
      ]
    },
    {
      "metadata": {
        "id": "iDuG94H-C122",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features, labels = next(iter(train_dataset))\n",
        "\n",
        "features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E63mArnQaAGz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that like-features are grouped together, or *batched*. Each example row's fields are appended to the corresponding feature array. Change the `batch_size` to set the number of examples stored in these feature arrays.\n",
        "\n",
        "You can start to see some clusters by plotting a few features from the batch:"
      ]
    },
    {
      "metadata": {
        "id": "me5Wn-9FcyyO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(features['petal_length'],\n",
        "            features['sepal_length'],\n",
        "            c=labels,\n",
        "            cmap='viridis')\n",
        "\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Sepal length\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YlxpSyHlhT6M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To simplify the model building step, create a function to repackage the features dictionary into a single array with shape: `(batch_size, num_features)`.\n",
        "\n",
        "This function uses the [tf.stack](https://www.tensorflow.org/api_docs/python/tf/stack) method which takes values from a list of tensors and creates a combined tensor at the specified dimension."
      ]
    },
    {
      "metadata": {
        "id": "jm932WINcaGU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pack_features_vector(features, labels):\n",
        "  \"\"\"Pack the features into a single array.\"\"\"\n",
        "  features = tf.stack(list(features.values()), axis=1)\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1Vuph_eDl8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then use the [tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/dataset/map) method to pack the `features` of each `(features,label)` pair into the training dataset:"
      ]
    },
    {
      "metadata": {
        "id": "ZbDkzGZIkpXf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(pack_features_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLy0Q1xCldVO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The features element of the `Dataset` are now arrays with shape `(batch_size, num_features)`. Let's look at the first few examples:"
      ]
    },
    {
      "metadata": {
        "id": "kex9ibEek6Tr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features, labels = next(iter(train_dataset))\n",
        "\n",
        "print(features[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsaVrtNM3Tx5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Select the type of model\n"
      ]
    },
    {
      "metadata": {
        "id": "W23DIMVPQEBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a model using Keras\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2fZ6oL2ig3ZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(3)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wFKnhWCpDSS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using the model\n",
        "\n",
        "Let's have a quick look at what this model does to a batch of features:"
      ]
    },
    {
      "metadata": {
        "id": "xe6SQ5NrpB-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model(features)\n",
        "predictions[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxyXOhwVr5S3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, each example returns a [logit](https://developers.google.com/machine-learning/crash-course/glossary#logit) for each class. \n",
        "\n",
        "To convert these logits to a probability for each class, use the [softmax](https://developers.google.com/machine-learning/crash-course/glossary#softmax) function:"
      ]
    },
    {
      "metadata": {
        "id": "_tRwHZmTNTX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(predictions[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRZmchElo481",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Taking the `tf.argmax` across classes gives us the predicted class index. But, the model hasn't been trained yet, so these aren't good predictions."
      ]
    },
    {
      "metadata": {
        "id": "-Jzm_GoErz8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Prediction: {}\".format(tf.argmax(predictions, axis=1)))\n",
        "print(\"    Labels: {}\".format(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vzq2E5J2QMtw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RaKp8aEjKX6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the loss and gradient function\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tMAT4DcMPwI-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(model, x, y):\n",
        "  y_ = model(x)\n",
        "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
        "\n",
        "\n",
        "l = loss(model, features, labels)\n",
        "print(\"Loss test: {}\".format(l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x57HcKWhKkei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOxFimtlKruu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create an optimizer\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8xxi2NNGKwG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "\n",
        "global_step = tf.train.get_or_create_global_step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rxRNTFVe56RG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_value, grads = grad(model, features, labels)\n",
        "\n",
        "print(\"Step: {}, Initial Loss: {}\".format(global_step.numpy(),\n",
        "                                          loss_value.numpy()))\n",
        "\n",
        "optimizer.apply_gradients(zip(grads, model.variables), global_step)\n",
        "\n",
        "print(\"Step: {},         Loss: {}\".format(global_step.numpy(),\n",
        "                                          loss(model, features, labels).numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Y2VSELvwAvW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AIgulGRUhpto",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Note: Rerunning this cell uses the same model variables\n",
        "\n",
        "# keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 201\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tfe.metrics.Mean()\n",
        "  epoch_accuracy = tfe.metrics.Accuracy()\n",
        "\n",
        "  # Training loop - using batches of 32\n",
        "  for x, y in train_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value, grads = grad(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.variables),\n",
        "                              global_step)\n",
        "\n",
        "    # Track progress\n",
        "    epoch_loss_avg(loss_value)  # add current batch loss\n",
        "    # compare predicted label to actual label\n",
        "    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n",
        "\n",
        "  # end epoch\n",
        "  train_loss_results.append(epoch_loss_avg.result())\n",
        "  train_accuracy_results.append(epoch_accuracy.result())\n",
        "  \n",
        "  if epoch % 50 == 0:\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2FQHVUnm_rjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the loss function over time"
      ]
    },
    {
      "metadata": {
        "id": "j3wdbmtLVTyr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "agjvNd2iUGFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].plot(train_loss_results)\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zg8GoMZhLpGH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model's effectiveness\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z-EvK7hGL0d8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup the test dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ps3_9dJ3Lodk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "\n",
        "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
        "                                  origin=test_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SRMWCu30bnxH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset = tf.contrib.data.make_csv_dataset(\n",
        "    train_dataset_fp,\n",
        "    batch_size, \n",
        "    column_names=column_names,\n",
        "    label_name='species',\n",
        "    num_epochs=1,\n",
        "    shuffle=False)\n",
        "\n",
        "test_dataset = test_dataset.map(pack_features_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFuOKXJdMAdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model on the test dataset\n",
        "\n",
        "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/#epoch) of the test data. In the following code cell, we iterate over each example in the test set and compare the model's prediction against the actual label. This is used to measure the model's accuracy across the entire test set."
      ]
    },
    {
      "metadata": {
        "id": "Tw03-MK1cYId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_accuracy = tfe.metrics.Accuracy()\n",
        "\n",
        "for (x, y) in test_dataset:\n",
        "  logits = model(x)\n",
        "  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "  test_accuracy(prediction, y)\n",
        "\n",
        "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HcKEZMtCOeK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see on the last batch, for example, the model is usually correct:"
      ]
    },
    {
      "metadata": {
        "id": "uNwt2eMeOane",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.stack([y,prediction],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Li2r1tYvW7S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use the trained model to make predictions\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kesTS5Lzv-M2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_dataset = tf.convert_to_tensor([\n",
        "    [5.1, 3.3, 1.7, 0.5,],\n",
        "    [5.9, 3.0, 4.2, 1.5,],\n",
        "    [6.9, 3.1, 5.4, 2.1]\n",
        "])\n",
        "\n",
        "predictions = model(predict_dataset)\n",
        "\n",
        "for i, logits in enumerate(predictions):\n",
        "  class_idx = tf.argmax(logits).numpy()\n",
        "  p = tf.nn.softmax(logits)[class_idx]\n",
        "  name = class_names[class_idx]\n",
        "  print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}