{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris_graph_execution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "JtEZ1pCPn--z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Iris Dataset with Graph Execution\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yNr7H-AIoLOR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup program"
      ]
    },
    {
      "metadata": {
        "id": "jBmKxLVy9Uhg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Iris Setosa', 'Iris Versicolor', 'Iris Virginica']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1J3AuPBT9gyR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configure imports\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g4Wzg69bnwK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Px6KAg0Jowz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import and parse the training dataset\n",
        "\n",
        "\n",
        "### Download the dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J6c7uEU9rjRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33632bfc-6c1a-40dd-e1b4-41cdc17381d0"
      },
      "cell_type": "code",
      "source": [
        "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "\n",
        "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
        "                                           origin=train_dataset_url)\n",
        "\n",
        "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local copy of the dataset file: /content/.keras/datasets/iris_training.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qnX1-aLors4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inspect the data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FQvb_JYdrpPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "789a4592-bc85-4d25-b296-19267e415e9e"
      },
      "cell_type": "code",
      "source": [
        "!head -n5 {train_dataset_fp}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120,4,setosa,versicolor,virginica\r\n",
            "6.4,2.8,5.6,2.2,2\r\n",
            "5.0,2.3,3.3,1.0,1\r\n",
            "4.9,2.5,4.5,1.7,2\r\n",
            "4.9,3.1,1.5,0.1,0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dqPkQExM2Pwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parse the dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "2y4OgiIz2CVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(train_dataset_fp, names=CSV_COLUMN_NAMES, header=0)\n",
        "y_name='Species'\n",
        "train_x, train_y = train, train.pop(y_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBGYOBS7zfdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the training tf.data.Dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7YYQUa1Hz2pP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    # Feature columns describe how to use the input.\n",
        "    my_feature_columns = []\n",
        "    for key in train_x.keys():\n",
        "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsaVrtNM3Tx5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Select the type of model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "W23DIMVPQEBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a model using Keras\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2fZ6oL2ig3ZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    # Build 2 hidden layer DNN with 10, 10 units respectively.\n",
        "    classifier = tf.estimator.DNNClassifier(\n",
        "        feature_columns=my_feature_columns,\n",
        "        # Two hidden layers of 10 nodes each.\n",
        "        hidden_units=[10, 10],\n",
        "        # The model must choose between 3 classes.\n",
        "        n_classes=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vzq2E5J2QMtw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RaKp8aEjKX6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the loss and gradient function\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "x57HcKWhKkei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle, repeat, and batch the examples.\n",
        "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
        "\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOxFimtlKruu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create an optimizer\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8xxi2NNGKwG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size =100\n",
        "train_steps = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Y2VSELvwAvW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AIgulGRUhpto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5512cc6-8151-4087-e9c8-c84db13b71af"
      },
      "cell_type": "code",
      "source": [
        "    # Train the Model.\n",
        "    classifier.train(\n",
        "        input_fn=lambda:train_input_fn(train_x, train_y,\n",
        "                                                 batch_size),\n",
        "        steps=train_steps)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7fed5e993048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "2FQHVUnm_rjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the loss function over time"
      ]
    },
    {
      "metadata": {
        "id": "agjvNd2iUGFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zg8GoMZhLpGH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model's effectiveness\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z-EvK7hGL0d8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup the test dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ps3_9dJ3Lodk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "\n",
        "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
        "                                  origin=test_url)\n",
        "\n",
        "y_name='Species'\n",
        "test = pd.read_csv(test_fp, names=CSV_COLUMN_NAMES, header=0)\n",
        "test_x, test_y = test, test.pop(y_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFuOKXJdMAdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model on the test dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Tw03-MK1cYId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
        "    features=dict(features)\n",
        "    if labels is None:\n",
        "        # No labels, use only features.\n",
        "        inputs = features\n",
        "    else:\n",
        "        inputs = (features, labels)\n",
        "\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "\n",
        "    # Batch the examples\n",
        "    assert batch_size is not None, \"batch_size must not be None\"\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oS_kjMBufBYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "15668629-4654-498b-f105-c1269c768669"
      },
      "cell_type": "code",
      "source": [
        "    # Evaluate the model.\n",
        "    eval_result = classifier.evaluate(\n",
        "        input_fn=lambda:eval_input_fn(test_x, test_y,\n",
        "                                                batch_size))\n",
        "\n",
        "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy: 0.967\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z2N2P8blHZue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use the trained model to make predictions\n"
      ]
    },
    {
      "metadata": {
        "id": "NkjcB7FzHZTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "6d8655c9-ba91-4296-c3e3-9691ef9bab66"
      },
      "cell_type": "code",
      "source": [
        "    # Generate predictions from the model\n",
        "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
        "    predict_x = {\n",
        "        'SepalLength': [5.1, 5.9, 6.9],\n",
        "        'SepalWidth': [3.3, 3.0, 3.1],\n",
        "        'PetalLength': [1.7, 4.2, 5.4],\n",
        "        'PetalWidth': [0.5, 1.5, 2.1],\n",
        "    }\n",
        "\n",
        "    predictions = classifier.predict(\n",
        "        input_fn=lambda:eval_input_fn(predict_x,\n",
        "                                                labels=None,\n",
        "                                                batch_size=batch_size))\n",
        "\n",
        "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
        "\n",
        "    for pred_dict, expec in zip(predictions, expected):\n",
        "        class_id = pred_dict['class_ids'][0]\n",
        "        probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "        print(template.format(SPECIES[class_id],\n",
        "                              100 * probability, expec))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Prediction is \"Iris Setosa\" (99.8%), expected \"Setosa\"\n",
            "\n",
            "Prediction is \"Iris Versicolor\" (99.8%), expected \"Versicolor\"\n",
            "\n",
            "Prediction is \"Iris Virginica\" (97.0%), expected \"Virginica\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}