{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris_graph_execution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "JtEZ1pCPn--z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Iris Dataset with Graph Execution\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yNr7H-AIoLOR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup program"
      ]
    },
    {
      "metadata": {
        "id": "jBmKxLVy9Uhg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1J3AuPBT9gyR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configure imports\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g4Wzg69bnwK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Px6KAg0Jowz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import and parse the training dataset\n",
        "\n",
        "\n",
        "### Download the dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J6c7uEU9rjRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "train_dataset_path = tf.keras.utils.get_file(\n",
        "    fname=os.path.basename(train_dataset_url),\n",
        "    origin=train_dataset_url)\n",
        "\n",
        "print(\"Location of the training dataset file: {}\".format(train_dataset_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnX1-aLors4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inspect the data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FQvb_JYdrpPm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head -n5 {train_dataset_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqPkQExM2Pwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parse the dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "2y4OgiIz2CVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csv_column_names = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "\n",
        "train = pd.read_csv(train_dataset_path, names=csv_column_names, header=0)\n",
        "y_name='Species'\n",
        "train_x, train_y = train, train.pop(y_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBGYOBS7zfdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the training tf.data.Dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7YYQUa1Hz2pP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    # Feature columns describe how to use the input.\n",
        "    my_feature_columns = []\n",
        "    for key in train_x.keys():\n",
        "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsaVrtNM3Tx5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Select the type of model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "W23DIMVPQEBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a model using Keras\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2fZ6oL2ig3ZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    # Build 2 hidden layer DNN with 10, 10 units respectively.\n",
        "    classifier = tf.estimator.DNNClassifier(\n",
        "        feature_columns=my_feature_columns,\n",
        "        # Two hidden layers of 10 nodes each.\n",
        "        hidden_units=[10, 10],\n",
        "        # The model must choose between 3 classes.\n",
        "        n_classes=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vzq2E5J2QMtw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RaKp8aEjKX6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the loss and gradient function\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "x57HcKWhKkei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle, repeat, and batch the examples.\n",
        "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
        "\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOxFimtlKruu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create an optimizer\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8xxi2NNGKwG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size =100\n",
        "train_steps = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Y2VSELvwAvW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AIgulGRUhpto",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    # Train the Model.\n",
        "    classifier.train(\n",
        "        input_fn=lambda:train_input_fn(train_x, train_y,\n",
        "                                                 batch_size),\n",
        "        steps=train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2FQHVUnm_rjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the loss function over time"
      ]
    },
    {
      "metadata": {
        "id": "agjvNd2iUGFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zg8GoMZhLpGH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model's effectiveness\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z-EvK7hGL0d8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup the test dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ps3_9dJ3Lodk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "test_dataset_path = tf.keras.utils.get_file(\n",
        "    fname=os.path.basename(test_dataset_url),\n",
        "    origin=test_dataset_url)\n",
        "\n",
        "print(\"Location of the test dataset file: {}\".format(test_dataset_path))\n",
        "\n",
        "y_name='Species'\n",
        "test = pd.read_csv(test_dataset_path, names=csv_column_names, header=0)\n",
        "test_x, test_y = test, test.pop(y_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFuOKXJdMAdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model on the test dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Tw03-MK1cYId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
        "    features=dict(features)\n",
        "    if labels is None:\n",
        "        # No labels, use only features.\n",
        "        inputs = features\n",
        "    else:\n",
        "        inputs = (features, labels)\n",
        "\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "\n",
        "    # Batch the examples\n",
        "    assert batch_size is not None, \"batch_size must not be None\"\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oS_kjMBufBYO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    # Evaluate the model.\n",
        "    eval_result = classifier.evaluate(\n",
        "        input_fn=lambda:eval_input_fn(test_x, test_y,\n",
        "                                                batch_size))\n",
        "\n",
        "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2N2P8blHZue",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use the trained model to make predictions\n"
      ]
    },
    {
      "metadata": {
        "id": "NkjcB7FzHZTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SPECIES = ['Iris Setosa', 'Iris Versicolor', 'Iris Virginica']\n",
        " \n",
        "# Generate predictions from the model\n",
        "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
        "predict_x = {\n",
        "        'SepalLength': [5.1, 5.9, 6.9],\n",
        "        'SepalWidth': [3.3, 3.0, 3.1],\n",
        "        'PetalLength': [1.7, 4.2, 5.4],\n",
        "        'PetalWidth': [0.5, 1.5, 2.1],\n",
        "}\n",
        "\n",
        "predictions = classifier.predict(\n",
        "        input_fn=lambda:eval_input_fn(predict_x,\n",
        "                                                labels=None,\n",
        "                                                batch_size=batch_size))\n",
        "\n",
        "template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
        "\n",
        "for pred_dict, expec in zip(predictions, expected):\n",
        "        class_id = pred_dict['class_ids'][0]\n",
        "        probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "        print(template.format(SPECIES[class_id],\n",
        "                              100 * probability, expec))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}